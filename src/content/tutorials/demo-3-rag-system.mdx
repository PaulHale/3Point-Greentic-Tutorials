---
title: "Demo 3: RAG Knowledge System"
description: Create a full-stack RAG knowledge base with an Astro frontend, Greentic backend, and MCP tools for vector search.
difficulty: advanced
duration: "90 minutes"
prerequisites:
  - Completed Demo 1 and 2
  - Node.js 20+
  - Familiarity with React/Astro
  - OpenAI API key
order: 3
---

import CodeBlock from '@/components/tutorial/CodeBlock';
import Terminal from '@/components/tutorial/Terminal';
import Callout from '@/components/ui/Callout.astro';

# RAG Knowledge System

In this advanced tutorial, you'll build a complete Retrieval-Augmented Generation (RAG) system that includes:

- **Astro frontend** with embedded WebChat widget
- **Document ingestion** pipeline for your knowledge base
- **Vector search** using MCP tools
- **LLM-powered** question answering
- **Full deployment** configuration

## Architecture Overview

import { RAGArchitectureDiagram } from '@/components/diagrams/RAGArchitectureDiagram';

<RAGArchitectureDiagram client:visible className="my-6" />

## Part 1: Set Up the Project

Create your project structure:

<Terminal
  client:visible
  lines={[
    { type: 'command', content: 'mkdir rag-knowledge && cd rag-knowledge' },
    { type: 'command', content: 'mkdir -p frontend backend/flows backend/docs' },
  ]}
/>

Your project will look like this:

```
rag-knowledge/
├── frontend/           # Astro website
│   ├── src/
│   └── package.json
├── backend/
│   ├── flows/          # Greentic flows
│   ├── docs/           # Knowledge base documents
│   └── .env
└── docker-compose.yml
```

## Part 2: Create the Astro Frontend

### Initialise Astro Project

<Terminal
  client:visible
  lines={[
    { type: 'command', content: 'cd frontend' },
    { type: 'command', content: 'npm create astro@latest . -- --template minimal' },
    { type: 'output', content: '✓ Project created!' },
    { type: 'command', content: 'npm install @greentic/webchat react react-dom' },
  ]}
/>

### Create the Landing Page

<CodeBlock
  client:visible
  filename="frontend/src/pages/index.astro"
  language="astro"
  code={`---
import WebChat from '../components/WebChat.astro';
---

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width" />
    <title>Knowledge Base</title>
    <style>
      body {
        font-family: system-ui, sans-serif;
        margin: 0;
        min-height: 100vh;
        background: linear-gradient(135deg, #1a1b26 0%, #16161e 100%);
        color: #a9b1d6;
      }

      .hero {
        max-width: 800px;
        margin: 0 auto;
        padding: 4rem 2rem;
        text-align: center;
      }

      h1 {
        font-size: 3rem;
        background: linear-gradient(135deg, #2dd4bf, #06b6d4);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 1rem;
      }

      p {
        font-size: 1.25rem;
        opacity: 0.8;
        max-width: 600px;
        margin: 0 auto 2rem;
      }

      .features {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 1.5rem;
        margin-top: 3rem;
      }

      .feature {
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 12px;
        padding: 1.5rem;
        text-align: left;
      }

      .feature h3 {
        color: #2dd4bf;
        margin-bottom: 0.5rem;
      }
    </style>
  </head>
  <body>
    <div class="hero">
      <h1>Knowledge Base</h1>
      <p>
        Ask questions about our documentation and get instant, accurate answers
        powered by AI.
      </p>

      <div class="features">
        <div class="feature">
          <h3>Instant Answers</h3>
          <p>Get responses in seconds, not hours of searching.</p>
        </div>
        <div class="feature">
          <h3>Source Citations</h3>
          <p>Every answer includes links to source documents.</p>
        </div>
        <div class="feature">
          <h3>Always Learning</h3>
          <p>Continuously updated with the latest documentation.</p>
        </div>
      </div>
    </div>

    <WebChat />
  </body>
</html>`}
/>

### Create the WebChat Component

<CodeBlock
  client:visible
  filename="frontend/src/components/WebChat.astro"
  language="astro"
  code={`---
// WebChat configuration
const chatConfig = {
  endpoint: import.meta.env.GREENTIC_ENDPOINT || 'http://localhost:8080',
  tenant: import.meta.env.GREENTIC_TENANT || 'demo',
  flow: 'rag-knowledge',
  theme: {
    primary: '#2dd4bf',
    background: '#1a1b26',
    text: '#a9b1d6',
  },
};
---

<div id="greentic-webchat" data-config={JSON.stringify(chatConfig)}></div>

<script>
  import { initWebChat } from '@greentic/webchat';

  const container = document.getElementById('greentic-webchat');
  const config = JSON.parse(container?.dataset.config || '{}');

  initWebChat(container, {
    ...config,
    position: 'bottom-right',
    greeting: "Hi! I'm your knowledge base assistant. Ask me anything about our documentation.",
  });
</script>

<style>
  #greentic-webchat {
    position: fixed;
    bottom: 20px;
    right: 20px;
    z-index: 1000;
  }
</style>`}
/>

## Part 3: Create the RAG Flow

### The Main RAG Flow

<Terminal
  client:visible
  lines={[
    { type: 'command', content: 'cd ../backend' },
    { type: 'command', content: 'greentic-flow new --flow flows/rag-knowledge.ygtc --id rag-knowledge --type messaging --name "RAG Knowledge Base"' },
    { type: 'success', content: "✓ Created flows/rag-knowledge.ygtc" },
  ]}
/>

<CodeBlock
  client:visible
  filename="backend/flows/rag-knowledge.ygtc"
  language="yaml"
  code={`id: rag-knowledge
title: RAG Knowledge Base
description: RAG-powered knowledge base assistant
type: messaging
start: process_query

parameters:
  embedding_model: text-embedding-3-small
  chat_model: gpt-4o
  top_k: 5

nodes:
  process_query:
    templating.handlebars:
      text: "{{in.text}}"
    routing:
      - to: embed_query

  embed_query:
    mcp.exec:
      component: llm
      action: embed
      args:
        model: parameters.embedding_model
        input: "{{in.text}}"
    routing:
      - to: vector_search

  vector_search:
    mcp.exec:
      component: vector_db
      action: search
      args:
        embedding: embed_query.embedding
        top_k: parameters.top_k
        collection: knowledge-base
    routing:
      - to: generate_answer

  generate_answer:
    mcp.exec:
      component: llm
      action: chat
      args:
        model: parameters.chat_model
        temperature: 0.3
        system: |
          You are a helpful knowledge base assistant. Answer questions based
          ONLY on the provided context. If the answer isn't in the context,
          say "I don't have information about that in my knowledge base."

          Always cite your sources by mentioning the document title.

          Format your response with:
          1. A clear, direct answer
          2. Supporting details from the context
          3. Source citations at the end
        user: |
          Question: {{in.text}}

          Context from knowledge base:
          {{#vector_search.payload.results}}
          ---
          Document: {{title}}
          Content: {{content}}
          ---
          {{/vector_search.payload.results}}
    routing:
      - to: format_response

  format_response:
    templating.handlebars:
      text: |
        {{generate_answer.payload.text}}

        ---
        **Sources:**
        {{#vector_search.payload.results}}
        - {{title}}
        {{/vector_search.payload.results}}
    routing:
      - out: true`}
/>

### Configure MCP Tools

Create the MCP configuration for vector search:

<CodeBlock
  client:visible
  filename="backend/mcp-config.json"
  language="json"
  code={`{
  "tools": {
    "vector-search": {
      "type": "http",
      "endpoint": "http://vector-db:8000/search",
      "method": "POST",
      "schema": {
        "input": {
          "type": "object",
          "properties": {
            "embedding": {
              "type": "array",
              "items": { "type": "number" }
            },
            "top_k": {
              "type": "integer",
              "default": 5
            },
            "collection": {
              "type": "string"
            }
          },
          "required": ["embedding", "collection"]
        }
      }
    },
    "document-store": {
      "type": "http",
      "endpoint": "http://vector-db:8000/documents",
      "method": "POST"
    }
  }
}`}
/>

## Part 4: Document Ingestion Pipeline

Create a flow to ingest documents into the knowledge base:

<Terminal
  client:visible
  lines={[
    { type: 'command', content: 'greentic-flow new --flow flows/ingest-document.ygtc --id ingest-document --type job --name "Document Ingestion"' },
    { type: 'success', content: "✓ Created flows/ingest-document.ygtc" },
  ]}
/>

<CodeBlock
  client:visible
  filename="backend/flows/ingest-document.ygtc"
  language="yaml"
  code={`id: ingest-document
title: Document Ingestion
description: Ingest a document into the knowledge base
type: job
start: load_document

parameters:
  chunk_size: 1000
  chunk_overlap: 200
  embedding_model: text-embedding-3-small

nodes:
  load_document:
    mcp.exec:
      component: file_reader
      action: read
      args:
        path: "{{input.file_path}}"
    routing:
      - to: chunk_document

  chunk_document:
    mcp.exec:
      component: text_splitter
      action: split
      args:
        text: load_document.content
        chunk_size: parameters.chunk_size
        overlap: parameters.chunk_overlap
        separator: "\\n\\n"
    routing:
      - to: embed_chunks

  embed_chunks:
    mcp.exec:
      component: llm
      action: embed_batch
      args:
        model: parameters.embedding_model
        inputs: chunk_document.chunks
    routing:
      - to: store_vectors

  store_vectors:
    mcp.exec:
      component: vector_db
      action: upsert
      args:
        documents:
          title: "{{input.title}}"
          chunks: chunk_document.chunks
          embeddings: embed_chunks.embeddings
        collection: knowledge-base
    routing:
      - to: confirm

  confirm:
    templating.handlebars:
      text: |
        Document "{{input.title}}" ingested successfully!
        - Chunks created: {{chunk_document.chunks.length}}
        - Collection: knowledge-base
    routing:
      - out: true`}
/>

## Part 5: Docker Setup

Create a Docker Compose file for local development:

<CodeBlock
  client:visible
  filename="docker-compose.yml"
  language="yaml"
  code={`version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - GREENTIC_ENDPOINT=http://backend:8080
      - GREENTIC_TENANT=demo

  backend:
    image: greentic/runner:latest
    ports:
      - "8080:8080"
    volumes:
      - ./backend/flows:/flows
      - ./backend/mcp-config.json:/config/mcp.json
    environment:
      - OPENAI_API_KEY=\${OPENAI_API_KEY}
      - FLOW_DIR=/flows
      - MCP_CONFIG=/config/mcp.json

  vector-db:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

volumes:
  qdrant_data:`}
/>

## Part 6: Add Sample Documents

Create some markdown documents for your knowledge base:

<CodeBlock
  client:visible
  filename="backend/docs/getting-started.md"
  language="markdown"
  code={`# Getting Started with Our Product

## Introduction

Welcome to our product! This guide will help you get up and running quickly.

## Installation

1. Download the installer from our website
2. Run the installer with administrator privileges
3. Follow the setup wizard

## First Steps

After installation, launch the application and:

1. Create your account
2. Verify your email
3. Complete the onboarding tutorial

## Need Help?

Contact support at support@example.com or visit our help centre.`}
/>

<CodeBlock
  client:visible
  filename="backend/docs/api-reference.md"
  language="markdown"
  code={`# API Reference

## Authentication

All API requests require a valid API key in the header:

\`\`\`
Authorization: Bearer your-api-key
\`\`\`

## Endpoints

### GET /api/users

Returns a list of users.

**Parameters:**
- \`page\` (optional): Page number
- \`limit\` (optional): Results per page

### POST /api/users

Creates a new user.

**Body:**
\`\`\`json
{
  "name": "string",
  "email": "string"
}
\`\`\`

## Rate Limits

- 100 requests per minute for free tier
- 1000 requests per minute for paid tier`}
/>

## Part 7: Ingest Documents

Run the ingestion flow for each document:

<Terminal
  client:visible
  lines={[
    { type: 'command', content: 'greentic-flow doctor flows/' },
    { type: 'success', content: "✓ All flows valid" },
    { type: 'command', content: 'greentic-pack build --in . --gtpack-out dist/rag.gtpack' },
    { type: 'success', content: '✓ Pack built: dist/rag.gtpack' },
    { type: 'command', content: 'greentic-dev pack run --pack dist/rag.gtpack --input \'{"file_path": "docs/getting-started.md", "title": "Getting Started"}\' --flow ingest-document' },
    { type: 'output', content: 'Loading document...' },
    { type: 'output', content: 'Chunking: 3 chunks created' },
    { type: 'output', content: 'Embedding chunks...' },
    { type: 'output', content: 'Storing vectors...' },
    { type: 'success', content: 'Document "Getting Started" ingested successfully!' },
  ]}
/>

## Part 8: Test the System

Start all services:

<Terminal
  client:visible
  lines={[
    { type: 'command', content: 'docker-compose up -d' },
    { type: 'output', content: 'Creating network "rag-knowledge_default"' },
    { type: 'output', content: 'Creating rag-knowledge_vector-db_1 ... done' },
    { type: 'output', content: 'Creating rag-knowledge_backend_1 ... done' },
    { type: 'output', content: 'Creating rag-knowledge_frontend_1 ... done' },
  ]}
/>

Open http://localhost:3000 and try asking questions:

- "How do I install the product?"
- "What are the API rate limits?"
- "How do I authenticate API requests?"

<Callout type="success" title="Working RAG System">
  You now have a fully functional RAG knowledge base! The system retrieves relevant documents and generates accurate, cited answers.
</Callout>

## Part 9: Production Deployment

For production, consider:

### Security

<CodeBlock
  client:visible
  filename="backend/.env.production"
  language="bash"
  code={`# Use environment variables, not hardcoded keys
OPENAI_API_KEY=\${OPENAI_API_KEY}

# Enable authentication
AUTH_ENABLED=true
AUTH_PROVIDER=oauth

# TLS configuration
TLS_ENABLED=true
TLS_CERT_PATH=/certs/cert.pem
TLS_KEY_PATH=/certs/key.pem`}
/>

### Scaling

- Use managed vector database (Pinecone, Weaviate Cloud)
- Deploy multiple runner instances behind a load balancer
- Cache embeddings for common queries
- Monitor with OpenTelemetry

## What You Learned

Congratulations! You've built a production-ready RAG system:

- ✅ Astro frontend with WebChat integration
- ✅ RAG flow with vector search via `mcp.exec`
- ✅ Document ingestion pipeline
- ✅ MCP tool configuration
- ✅ Docker deployment setup
- ✅ Production considerations

## Next Steps

- **[Messaging Channels](/docs/messaging/overview)** — Add Teams, Slack support
- **[MCP Deep Dive](/docs/mcp/overview)** — Create custom tools
- **[Security Guide](/docs/architecture/security)** — Production hardening
